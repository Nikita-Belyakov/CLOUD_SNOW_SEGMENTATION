{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3e62b7",
   "metadata": {},
   "source": [
    "# NOTEBOOK FOR  TRAINING MANET MODEL ON ELECTRO-L №2 DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c47ae7",
   "metadata": {},
   "source": [
    "## IMPORT ALL REQUIRED PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12443e93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12443e93",
    "outputId": "d0254682-8605-49ae-896a-a488e28f909e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import pandas as pd\n",
    "import patoolib\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from timeit import default_timer as timer \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from IPython.display import clear_output\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import random\n",
    "import glob\n",
    "import tifffile as tff\n",
    "from skimage.transform import resize as interp_resize\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.optim as optim\n",
    "import rasterio\n",
    "from ranger21 import Ranger21\n",
    "import kornia as K\n",
    "from kornia.augmentation.container import AugmentationSequential\n",
    "from kornia.augmentation import (\n",
    "    RandomAffine,\n",
    "    RandomElasticTransform,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomPerspective,\n",
    "    RandomRotation,\n",
    "    RandomVerticalFlip)\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea894fa",
   "metadata": {},
   "source": [
    "## Initilize your current directory and device (CUDA recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ced7a82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ced7a82",
    "outputId": "db4c076d-e0a5-47e6-b7f3-639fc4f7b751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current dir of this .ipynb file C:\\Users\\nikita.belyakov\\Documents\\GitHub\\CLOUD_SNOW_SEGMENTATION\\RES_4_KM\\MANet_training\n",
      "Your device: cuda\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=1e7)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "your_current_dir = os.getcwd()\n",
    "print('Your current dir of this .ipynb file',your_current_dir)\n",
    "print('Your device:',device)\n",
    "your_current_dir = your_current_dir.replace('training_model_utils','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2880aa0a",
   "metadata": {},
   "source": [
    "## Set random seeds for stability everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1fb9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd154fbb",
   "metadata": {},
   "source": [
    "## Define functions for preprocessing and postprocessing multispectral data as pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2c98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_img(xb, idx=0):\n",
    "    img = np.array(xb.squeeze(0))\n",
    "    return np.array(img.transpose((1,2,0))*255)\n",
    "def transpose_patch(patch):\n",
    "    tr = np.array(patch).astype(np.uint8).transpose((2,0,1))\n",
    "    return tr\n",
    "def predb_to_mask(predb,idx=0):\n",
    "    p = torch.functional.F.softmax(predb.squeeze(0), 0)\n",
    "    return p.argmax(0).cpu()\n",
    "def inverse_normalize(tensor,mean,std,num_shannels =5):\n",
    "    tensor = tensor.detach().cpu().numpy()\n",
    "    # unnormalize the RGB channels\n",
    "    for i in range(num_shannels):\n",
    "        tensor[i] = (tensor[i] * std[i]) + mean[i]\n",
    "    # clip values to [0, 1] range\n",
    "    tensor = np.clip(tensor, 0, 1)\n",
    "    # convert back to uint8\n",
    "    tensor = (tensor * 255).astype(np.uint8)\n",
    "    return tensor\n",
    "def open_sample_as_pil_no_norm(datacube):\n",
    "    inversed_rgb = (datacube[0:3])\n",
    "    rgb = np.dstack((inversed_rgb[2,:,:],inversed_rgb[1,:,:],inversed_rgb[0,:,:]))*255\n",
    "    return rgb.astype('uint8')\n",
    "def open_DEM(datacube):\n",
    "    dem_arr = datacube[5]#*255\n",
    "    return dem_arr.numpy()#.astype('uint8')\n",
    "def open_mask_as_pil(seglabel):\n",
    "    mask = (seglabel.numpy())\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6603c",
   "metadata": {},
   "source": [
    "## MODEL INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bc073fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets initialize  architecture of segmentation FOR ALL 12 channels\n",
    "model = smp.MAnet(\n",
    "    encoder_name='efficientnet-b0', \n",
    "    in_channels=12,\n",
    "    classes=3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e03264d",
   "metadata": {},
   "source": [
    "## CREATE A DATASET CLASS WITH ELECTRO-L №2 with masks from GOES, METEOSAT & Terra/MODIS for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80a43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELECTRO_L2_Dataset_(Dataset):\n",
    "    def __init__(self, stack_dir_list,aux_dir, pytorch=True, include_BT = True,nonempty_mode = True):\n",
    "        super().__init__()\n",
    "        self.pytorch = pytorch   \n",
    "        self.nonempty_mode = nonempty_mode\n",
    "        self.include_BT = include_BT\n",
    "        self.stack_dir_list = stack_dir_list\n",
    "        self.non_empty_list = []\n",
    "        self.snowy_list = []\n",
    "        self.stack_dirs = []\n",
    "        self.stack_dirs_BT = []\n",
    "        self.all_snow_flags = []\n",
    "        self.non_empty_snow_idxs = []\n",
    "        self.non_empty_aux = []\n",
    "        self.aux_dir = aux_dir\n",
    "        total_len = 0\n",
    "        for i in range(len(self.stack_dir_list)): # iterate over number of pictures in dataset (1 now)\n",
    "            stack_dir = self.stack_dir_list[i]\n",
    "            stack_dir_BT = self.stack_dir_list[i].replace('rgb', 'BT').replace('folder_ZSA','folder_BT')\n",
    "            non_empty_idxs_dir = self.stack_dir_list[i].replace('all_patch_folder_ZSA','nonempty_idxs_folder')\n",
    "            non_empty_idxs_dir = non_empty_idxs_dir.replace('patches_rgb','nonempty_idxs')\n",
    "            snowy_idxs_dir = non_empty_idxs_dir.replace('nonempty_idxs_folder_ZSA', 'snowy_idxs_folder')\n",
    "            snowy_idxs_dir = snowy_idxs_dir.replace('nonempty', 'snowy')\n",
    "            snow_flag_array = np.load(snowy_idxs_dir+'.npy')\n",
    "            self.all_snow_flags.append(snow_flag_array)\n",
    "            if self.nonempty_mode:\n",
    "                non_empty_idxs = np.load(non_empty_idxs_dir+'.npy')\n",
    "                snow_idxs_nonempty = np.zeros(len(snow_flag_array))\n",
    "                snow_idxs_nonempty[non_empty_idxs]=1\n",
    "                snow_idxs_nonempty = (snow_idxs_nonempty*snow_flag_array).astype(np.uint8)\n",
    "                for j in range(len(non_empty_idxs)):\n",
    "                    idx_ = non_empty_idxs[j]\n",
    "                    if snow_idxs_nonempty[idx_] == 1:\n",
    "                        self.non_empty_snow_idxs.append(total_len+j)\n",
    "                    patch_file = f\"{stack_dir}/patch_{idx_}.tif\"\n",
    "                    self.stack_dirs.append(patch_file)\n",
    "                    patch_file_BT = f\"{stack_dir_BT}/patch_{idx_}.tif\"\n",
    "                    self.stack_dirs_BT.append(patch_file_BT)\n",
    "                    self.non_empty_aux.append(idx_)\n",
    "                total_len = total_len+len(non_empty_idxs)\n",
    "            else:\n",
    "                patch_files = glob.glob(stack_dir+'/*')\n",
    "                patch_files_BT = glob.glob(stack_dir_BT+'/*')\n",
    "                for patch_file in patch_files:\n",
    "                    self.stack_dirs.append(patch_file)\n",
    "                for patch_file_BT in patch_files_BT:\n",
    "                    self.stack_dirs_BT.append(patch_file_BT) \n",
    "        self.all_snow_flags_nonempty =np.zeros(total_len).astype(np.uint8)\n",
    "        self.all_snow_flags_nonempty[self.non_empty_snow_idxs]=1\n",
    "        self.all_snow_flags = np.array(self.all_snow_flags).flatten()\n",
    "        \n",
    "    def __len__(self):\n",
    "        num_patches = int((len(self.stack_dirs))) \n",
    "        return num_patches\n",
    "    \n",
    "    def open_rgb_normed(self, idx, invert=False):\n",
    "        patch_file = self.stack_dirs[idx]\n",
    "        patch = tff.imread(patch_file)\n",
    "        r,g,b = patch[:,:,0]/255, patch[:,:,1]/255, patch[:,:,2]/255\n",
    "        patch_normed  = np.dstack([b, g ,r])\n",
    "        return patch_normed \n",
    "    \n",
    "    def open_BT_normed(self, idx, invert=False):\n",
    "        patch_file_BT = self.stack_dirs_BT[idx]\n",
    "        patch_BT = tff.imread(patch_file_BT)\n",
    "        return patch_BT \n",
    "    \n",
    "    def open_aux(self, idx, invert=False):\n",
    "        patch_file = self.aux_dir+'/patch_'+str(self.non_empty_aux[idx])+'.tif'\n",
    "        patch = tff.imread(patch_file)\n",
    "        lon,lat,dem = patch[:,:,0],patch[:,:,1],patch[:,:,2]\n",
    "        lon,lat = (lon+180.0)/360.0,(lat+90.0)/180.0\n",
    "        dem = dem/10000\n",
    "        lonlatdem_normed = np.dstack([lon,lat,dem])\n",
    "        return lonlatdem_normed\n",
    "    \n",
    "    def open_mask(self, idx):\n",
    "        patch_file = self.stack_dirs[idx]\n",
    "        mask_file = patch_file.replace('rgb','masks').replace('tif','jpg').replace('all_patch_folder_ZSA','all_masks_folder')\n",
    "        mask = tff.imread(mask_file)\n",
    "        snow_mask = mask[:,:,1]//255 #  1 - snow class\n",
    "        cloud_mask = mask[:,:,0] # 2 - cloud class\n",
    "        cloud_mask[cloud_mask!=0] = 2\n",
    "        bg = mask[:,:,2]\n",
    "        bin_mask = snow_mask + cloud_mask\n",
    "        return bin_mask\n",
    "    \n",
    "    def open_as_pil(self, idx):\n",
    "        patch_file = self.stack_dirs[idx]\n",
    "        patch_image = tff.imread(patch_file)\n",
    "        return patch_image \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patch_file = self.stack_dirs[idx]\n",
    "        mask_file = patch_file.replace('rgb','masks').replace('tif','jpg').replace('all_patch_folder_ZSA','all_masks_folder')\n",
    "        mask = tff.imread(mask_file)\n",
    "        snow_mask = mask[:,:,1]//255 #  1 - snow class\n",
    "        cloud_mask = mask[:,:,0] # 2 - cloud class\n",
    "        cloud_mask[cloud_mask!=0]=2\n",
    "        bin_mask = snow_mask+cloud_mask\n",
    "        patch = tff.imread(patch_file)\n",
    "        r,g,b = patch[:,:,0]/255, patch[:,:,1]/255, patch[:,:,2]/255\n",
    "        aux_file = self.aux_dir+'/patch_'+str(self.non_empty_aux[idx])+'.tif'\n",
    "        aux = tff.imread(aux_file)\n",
    "        lon,lat,dem = aux[:,:,0],aux[:,:,1],aux[:,:,2]\n",
    "        lon,lat = (lon+180.0)/360.0,(lat+90.0)/180.0\n",
    "        dem = dem/10000\n",
    "        aux_normed = np.stack([lon, lat, dem])\n",
    "        patch_normed  = np.stack([b, g ,r])\n",
    "        if self.include_BT:\n",
    "            patch_file_BT = self.stack_dirs_BT[idx] \n",
    "            BT_normed = tff.imread(patch_file_BT).transpose(2,0,1)\n",
    "            full_stack = np.concatenate((patch_normed, BT_normed, aux_normed), axis=0)\n",
    "            # order of channels: b, g, r, BT4, BT5, BT6, BT7, BT8, BT9, lon, lat, dem\n",
    "            full_stack = torch.tensor(full_stack, dtype=torch.float32)\n",
    "        else:\n",
    "            full_stack = np.concatenate((patch_normed, aux_normed), axis=0)\n",
    "            full_stack = torch.tensor(full_stack, dtype=torch.float32)\n",
    "        return full_stack, torch.tensor(bin_mask).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215de827",
   "metadata": {},
   "source": [
    "## INITIALIZE ELECTRO-L №2 DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbdc5002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikita.belyakov\\Documents\\GitHub\\CLOUD_SNOW_SEGMENTATION\\RES_4_KM\\data_inference ['C:\\\\Users\\\\nikita.belyakov\\\\Documents\\\\GitHub\\\\CLOUD_SNOW_SEGMENTATION\\\\RES_4_KM\\\\data_inference/all_patch_folder_ZSA\\\\patches_rgb_electro_l2_20230115_1400']\n"
     ]
    }
   ],
   "source": [
    "data_current_dir = your_current_dir.replace('MANet_training','data_inference')\n",
    "stack_dir = glob.glob(data_current_dir+'/all_patch_folder_ZSA/*')\n",
    "aux_dir_l2 = data_current_dir + '/lon_lat_dem'\n",
    "Electro_ds = ELECTRO_L2_Dataset_(stack_dir_list=stack_dir, aux_dir = aux_dir_l2,nonempty_mode = True, include_BT = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc364604",
   "metadata": {},
   "source": [
    "## DEFINE A PIPELINE CLASS OF GEOMETRIC TRANSFORM ON GPU USING Kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66a5a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia as K\n",
    "from kornia.augmentation.container import AugmentationSequential\n",
    "class Geom_Augmentation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Geom_Augmentation, self).__init__()\n",
    "        # we define and cache our operators as class members\n",
    "        self.augs = AugmentationSequential(\n",
    "                    RandomVerticalFlip(p=1),\n",
    "                    RandomHorizontalFlip(p=1),\n",
    "                    RandomPerspective(0.25, sampling_method = 'area_preserving', p=1.),\n",
    "                    RandomAffine(degrees =(-85.0,85.0),translate = None,scale = (0.9, 1.1),resample=\"nearest\",shear = None,padding_mode=\"reflection\",align_corners=True,same_on_batch=False,keepdim=True,p=1),\n",
    "                    RandomElasticTransform(kernel_size=(33, 33), sigma=(6.0, 6.0), alpha=(1.0, 1.0), align_corners=True, resample='nearest', padding_mode='reflection', same_on_batch=False, p=1.0, keepdim=True),\n",
    "                    data_keys=['input', 'mask'], same_on_batch = False, random_apply = 2)                       \n",
    "    def forward(self, img: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # 2. apply geometric tranform\n",
    "        out = self.augs(img, mask)\n",
    "        img_out, mask_out = out[0],out[1]\n",
    "        return img_out, mask_out\n",
    "geom_augs = Geom_Augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ced59c",
   "metadata": {},
   "source": [
    "## USE WEIGHTED SAMPLER TO MAKE DATALOADER MORE BALANCED WITH SNOW PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2045fa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  tensor([ 1.0000, 22.4000])\n"
     ]
    }
   ],
   "source": [
    "#SAMPLER FOR ELECTRO L2 DS FOR FINETUNNING\n",
    "# Electro_ds_tr, Electro_ds_val  = torch.utils.data.random_split(Electro_ds, (0.8,0.2))\n",
    "non_empty_snow_patches_ds = Electro_ds.all_snow_flags_nonempty\n",
    "num_non_empty_snow_patches = len(non_empty_snow_patches_ds)\n",
    "bg_weight = num_non_empty_snow_patches/len(Electro_ds)\n",
    "snowy_weight = 1 - num_non_empty_snow_patches/len(Electro_ds)\n",
    "batch_size = 16\n",
    "_, counts = torch.unique(torch.tensor(non_empty_snow_patches_ds), return_counts=True)\n",
    "weights = counts.max() / counts\n",
    "print(\"Weights: \", weights)\n",
    "weight_for_sampler_l2 = []  # Every sample must have a weight\n",
    "for snow_flag in non_empty_snow_patches_ds:\n",
    "    weight_for_sampler_l2.append(weights[snow_flag].item())\n",
    "sampler_l2 = WeightedRandomSampler(torch.tensor(weight_for_sampler_l2), len(Electro_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75f658a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16 #16 # try to set as max as possible\n",
    "train_dl_ = DataLoader(Electro_ds, batch_size=batch_size, sampler = sampler_l2)\n",
    "len(Electro_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b6cec",
   "metadata": {},
   "source": [
    "## Define function for train 1 epoch and saving best model according IoU metric value on validation subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "YqeHetID_8jy",
   "metadata": {
    "id": "YqeHetID_8jy"
   },
   "outputs": [],
   "source": [
    "def save_best_model(model,epoch,path = your_current_dir.replace('MANet_training','models/MANet_ep_')):\n",
    "        model_copy = deepcopy(model)\n",
    "        best_model = model_copy\n",
    "        best_model_name = path+str(epoch)\n",
    "        torch.save(best_model.state_dict(),best_model_name)\n",
    "        print('best model is on epoch =',epoch)\n",
    "        return best_model\n",
    "def train_ep(model, train_dataload, dice, focal, focal_alpha, optimizer,ep_i, best_train_iou, scheduler = None):    \n",
    "    model.cuda()\n",
    "    print('epoch_n =',ep_i)\n",
    "    model.train(True)  # Set train mode = true\n",
    "    step = 0\n",
    "    train_loss = 0\n",
    "    #initialize metrics\n",
    "    train_f1_score = torchmetrics.F1Score(num_classes=3, task = 'multiclass', average = 'macro').cuda()\n",
    "    train_iou_score = torchmetrics.JaccardIndex(num_classes=3, task = 'multiclass').cuda()\n",
    "    train_f1_score_sep = torchmetrics.F1Score(num_classes=3, task = 'multiclass', average = None).cuda()\n",
    "    train_iou_score_sep = torchmetrics.JaccardIndex(num_classes=3, task = 'multiclass', average = None).cuda()\n",
    "    # COMMENTED CODE IS FOR ADDING VALIDATION STEP IN A TRAINING LOOP\n",
    "#     val_f1_score = torchmetrics.F1Score(num_classes=3, task = 'multiclass', average = 'macro').cuda()\n",
    "#     val_iou_score = torchmetrics.JaccardIndex(num_classes=3, task = 'multiclass').cuda()\n",
    "#     val_f1_score_sep = torchmetrics.F1Score(num_classes=3, task = 'multiclass', average = None).cuda()\n",
    "#     val_iou_score_sep = torchmetrics.JaccardIndex(num_classes=3, task = 'multiclass', average = None).cuda()\n",
    "#     val_acc = torchmetrics.Accuracy(num_classes=3, task = 'multiclass').cuda()\n",
    "    # iterate over data\n",
    "    print('-----------training process---------')\n",
    "    for x,y in tqdm(train_dataload): \n",
    "        x = torch.tensor(x).type(torch.float32).cuda()\n",
    "        y = torch.tensor(y).type(torch.float32).cuda()\n",
    "        #add geom augmentations on GPU from kornia\n",
    "        x, y = geom_augs(x, y.unsqueeze(1)) #convert labels to float32 for kornia !!!!\n",
    "        y = y.type(torch.LongTensor).squeeze(1).cuda() #convert labels to Long again for model !!!!\n",
    "        step += 1\n",
    "        # vector graph of training with grad on CUDA\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        output = torch.functional.F.softmax(output, 1)\n",
    "        predictions = output.argmax(dim=1).cuda()\n",
    "        loss = (1-focal_alpha)*dice(output, y)+focal_alpha*focal(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # other things can be done on CPU\n",
    "        loss = loss.detach()\n",
    "        predictions, y = predictions.detach(), y.detach()\n",
    "        train_f1_score.update(predictions, y)\n",
    "        train_iou_score.update(predictions, y)\n",
    "        train_f1_score_sep.update(predictions, y)\n",
    "        train_iou_score_sep.update(predictions, y)\n",
    "        train_loss=train_loss+loss\n",
    "        x,y = None, None\n",
    "        # need for torch.no_grad in this training pass\n",
    "        if scheduler!=None:\n",
    "            scheduler.step()\n",
    "    train_loss = train_loss.cpu()/len(train_dataload)\n",
    "    print('after training epoch mean train loss =',train_loss)\n",
    "    # Compute the train metrics for the epoch\n",
    "    train_f1 = train_f1_score.compute()\n",
    "    train_iou = train_iou_score.compute()\n",
    "    train_f1_sep = train_f1_score_sep.compute()\n",
    "    train_iou_sep = train_iou_score_sep.compute()\n",
    "\n",
    "    # Reset the train metrics objects for the next epoch\n",
    "    train_f1_score.reset()\n",
    "    train_iou_score.reset()\n",
    "    train_f1_score_sep.reset()\n",
    "    train_iou_score_sep.reset()\n",
    "# # Inside your epoch training loop, after each batch is processed, compute the metrics on the batch predictions and ground truth\n",
    "#     with torch.no_grad():\n",
    "#         val_loss = 0\n",
    "#         print('-----------validation process---------')\n",
    "#         for x,y in tqdm(valid_dataload):\n",
    "#             x = x.cuda()#.cuda()\n",
    "#             y = y.type(torch.LongTensor).cuda()\n",
    "#             output = model(x)\n",
    "#             output = torch.functional.F.softmax(output, 1)\n",
    "#             # Assuming output has shape (batch_size, num_classes, height, width)\n",
    "#             # Convert the output to predictions by taking the argmax along the channel dimension\n",
    "#             predictions = output.argmax(dim=1).cuda()\n",
    "#             valid_loss = (1-focal_alpha)*dice(output, y)+focal_alpha*focal(output, y)\n",
    "#             val_loss = val_loss+valid_loss\n",
    "#             val_f1_score.update(predictions, y)\n",
    "#             val_iou_score.update(predictions, y)\n",
    "#             val_f1_score_sep.update(predictions, y)\n",
    "#             val_iou_score_sep.update(predictions, y)\n",
    "#             val_acc.update(predictions, y)\n",
    "#             x,y = None, None\n",
    "#         val_f1 = val_f1_score.compute()\n",
    "#         val_iou = val_iou_score.compute()\n",
    "#         val_acc_ = val_acc.compute()\n",
    "#         #calculate the same metrics seperately for each class\n",
    "#         val_f1_sep = val_f1_score_sep.compute()\n",
    "#         val_iou_sep = val_iou_score_sep.compute()\n",
    "#         FAR = 1 - val_acc_\n",
    "            \n",
    "#         # Compute the validation metrics for the epoch\n",
    "#         val_loss = val_loss.cpu()/(len(valid_dataload))\n",
    "#         print('valid loss =',val_loss)\n",
    "\n",
    "#         # Reset the validation metrics objects for the next epoch\n",
    "#         val_f1_score.reset()\n",
    "#         val_iou_score.reset()\n",
    "#         val_f1_score_sep.reset()\n",
    "#         val_iou_score_sep.reset()\n",
    "#         val_acc.reset()\n",
    "        # Print the F1 score and IoU for the current epoch on the train and validation sets\n",
    "    print('######### METRICS AFTER TRAINING EPOCH #########')\n",
    "    print(f\"Epoch {ep_i}, Train F1 score: {train_f1:.4f}, Train IoU: {train_iou:.4f}\")\n",
    "    print(f\"Train F1 score for each class: {train_f1_sep.cpu().numpy()}, \\nTrain IoU for each class: {train_iou_sep.cpu().numpy()}\")\n",
    "#         print(f\"Test FAR: {FAR:.4f}\")\n",
    "#         print(f\"Test F1 score for each class: {val_f1_sep.cpu().numpy()}, \\nTest IoU for each class: {val_iou_sep.cpu().numpy()}\")\n",
    "    if (best_train_iou<train_iou) or (train_iou>0.7):\n",
    "        save_best_model(model,ep_i)\n",
    "        print(\" model updated\")\n",
    "    return train_f1.cpu(),train_iou.cpu(), train_loss.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812c150",
   "metadata": {},
   "source": [
    "## Train loop MANet ELECTRO-L №2 data with Ranger21 optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92c52f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger21 optimizer ready with following settings:\n",
      "\n",
      "Core optimizer = AdamW\n",
      "Learning rate of 0.001\n",
      "\n",
      "Important - num_epochs of training = ** 2 epochs **\n",
      "please confirm this is correct or warmup and warmdown will be off\n",
      "\n",
      "Warm-up: linear warmup, over 3 iterations\n",
      "\n",
      "Lookahead active, merging every 5 steps, with blend factor of 0.5\n",
      "Norm Loss active, factor = 0.0001\n",
      "Stable weight decay of 0.0001\n",
      "Gradient Centralization = On\n",
      "\n",
      "Adaptive Gradient Clipping = True\n",
      "\tclipping value of 0.01\n",
      "\tsteps for clipping = 0.001\n",
      "\n",
      "Warm-down: Linear warmdown, starting at 72.0%, iteration 11 of 16\n",
      "warm down will decay until 3e-05 lr\n",
      "epoch_n = 0\n",
      "-----------training process---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▌                                                                         | 1/8 [00:01<00:13,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params size saved\n",
      "total param groups = 1\n",
      "total params in groups = 307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 4/8 [00:07<00:07,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Ranger21 update = Warmup complete - lr set to 0.001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:13<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training epoch mean train loss = tensor(0.6279)\n",
      "######### METRICS AFTER TRAINING EPOCH #########\n",
      "Epoch 0, Train F1 score: 0.4646, Train IoU: 0.3458\n",
      "Train F1 score for each class: [0.7271696  0.06090691 0.60586756], \n",
      "Train IoU for each class: [0.5713012  0.03141    0.43458396]\n",
      "epoch_n = 1\n",
      "-----------training process---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████▌                                                    | 3/8 [00:05<00:09,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Ranger21 update: Warmdown starting now.  Current iteration = 11....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:13<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in warmdown - lr below min lr. current lr = 2.999999999999997e-05\n",
      "auto handling but please report issue!\n",
      "after training epoch mean train loss = tensor(0.6142)\n",
      "######### METRICS AFTER TRAINING EPOCH #########\n",
      "Epoch 1, Train F1 score: 0.4887, Train IoU: 0.3784\n",
      "Train F1 score for each class: [0.76564634 0.03796589 0.66263616], \n",
      "Train IoU for each class: [0.62028116 0.01935027 0.49547938]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# continue finetunning training process on Electro-L № 2 ds with Ranger21 optimizer\n",
    "from ranger21 import Ranger21\n",
    "max_ep_num = 2 # can be set up more \n",
    "best_train_iou_ = 0.5\n",
    "cur_ep = 0\n",
    "lr, weight_decay = 1e-3, 1e-4\n",
    "#optimizer = optim.AdamW(model.parameters(),lr = 1e-3)\n",
    "#scheduler = CosineAnnealingLR(optimizer, T_max=max_ep_num)\n",
    "optimizer_Ranger21 =  Ranger21(model.parameters(), lr = lr, weight_decay = weight_decay,\n",
    "                                num_epochs = max_ep_num,num_batches_per_epoch = len(train_dl_))\n",
    "model.train()\n",
    "dice = smp.losses.DiceLoss(mode= 'multiclass')\n",
    "focal = smp.losses.FocalLoss(mode= 'multiclass', gamma = 2)\n",
    "focal_alpha = 0.7\n",
    "tr_f1_,tr_iou_,tr_loss_ = [],[best_train_iou_],[]\n",
    "for ep_i in range(cur_ep,cur_ep+max_ep_num):\n",
    "    train_f1_,train_iou_,train_loss_= train_ep(model, train_dl_, dice, focal,focal_alpha,optimizer_Ranger21,ep_i,best_train_iou_,scheduler=None)\n",
    "    tr_f1_.append(train_f1_)\n",
    "    tr_iou_.append(train_iou_)\n",
    "    best_train_iou_ = max(tr_iou_)\n",
    "    tr_loss_.append(train_loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae75993",
   "metadata": {},
   "source": [
    "## BLENDING SEVERAL MODELS via MODELS.SOUP TO RAISE QUALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5f700005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\ELECTRO_DATASET\\4_km_res\\L2/models/MAnet_Efficient_b0_12_inputs_4km_res_ep_216\n",
      "H:\\ELECTRO_DATASET\\4_km_res\\L2/models/MAnet_Efficient_b0_12_inputs_4km_res_ep_198\n",
      "H:\\ELECTRO_DATASET\\4_km_res\\L2/models/MAnet_Efficient_b0_12_inputs_4km_res_ep_175\n"
     ]
    }
   ],
   "source": [
    "# PREPARE A LIST OF SEVERAL MODELS WITH THE SAME ARCHITECTURE TO SOUP THEIR WEIGHTS\n",
    "model_path1 = '1st_model_dir'\n",
    "model_path2 = '2nd_model_dir'\n",
    "model_path3 = '3rd_model_dir'\n",
    "model_path_list = [model_path1,model_path2,model_path3]\n",
    "for i, model_path in enumerate(model_path_list):\n",
    "    print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "39a8d566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Uniform Soup Performance]\n",
      "-----------Testing process---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score: 0.8077, Test IoU: 0.7432\n",
      "Test FAR: 0.1093\n",
      "Test F1 score for each class(bg, snow, cloud): [0.8006987  0.74544346 0.8768815 ], \n",
      "Test IoU for each class(bg, snow, cloud): [0.6924185 0.7397989 0.7972732]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def uniform_soup(model, path, device = \"cpu\", by_name = False):\n",
    "    try:\n",
    "        import torch\n",
    "    except:\n",
    "        print(\"If you want to use 'Model Soup for Torch', please install 'torch'\")\n",
    "        return model\n",
    "        \n",
    "    if not isinstance(path, list):\n",
    "        path = [path]\n",
    "    model = model.to(device)\n",
    "    model_dict = model.state_dict()\n",
    "    soups = {key:[] for key in model_dict}\n",
    "    for i, model_path in enumerate(path):\n",
    "        weight = torch.load(model_path, map_location = device)\n",
    "        weight_dict = weight.state_dict() if hasattr(weight, \"state_dict\") else weight\n",
    "        if by_name:\n",
    "            weight_dict = {k:v for k, v in weight_dict.items() if k in model_dict}\n",
    "        for k, v in weight_dict.items():\n",
    "            soups[k].append(v)\n",
    "    if 0 < len(soups):\n",
    "        soups = {k:(torch.sum(torch.stack(v), axis = 0) / len(v)).type(v[0].dtype) for k, v in soups.items() if len(v) != 0}\n",
    "        model_dict.update(soups)\n",
    "        model.load_state_dict(model_dict)\n",
    "    return model\n",
    "print(\"\\n[Uniform Soup Performance]\")\n",
    "souped_model = uniform_soup(model, model_path_list, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f40cc7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "souped model saved!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MAnet(\n",
       "  (encoder): EfficientNetEncoder(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      12, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (3): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (4): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (6-7): 2 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (9-10): 2 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (11): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (12-14): 3 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (15): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (_swish): MemoryEfficientSwish()\n",
       "  )\n",
       "  (decoder): MAnetDecoder(\n",
       "    (center): PAB(\n",
       "      (top_conv): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (center_conv): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bottom_conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (map_softmax): Softmax(dim=1)\n",
       "      (out_conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): MFAB(\n",
       "        (hl_conv): Sequential(\n",
       "          (0): Conv2dReLU(\n",
       "            (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dReLU(\n",
       "            (0): Conv2d(320, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (SE_ll): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(112, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(7, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (SE_hl): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(112, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(7, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(224, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MFAB(\n",
       "        (hl_conv): Sequential(\n",
       "          (0): Conv2dReLU(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dReLU(\n",
       "            (0): Conv2d(256, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (SE_ll): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(40, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(2, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (SE_hl): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(40, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(2, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(80, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): MFAB(\n",
       "        (hl_conv): Sequential(\n",
       "          (0): Conv2dReLU(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dReLU(\n",
       "            (0): Conv2d(128, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (SE_ll): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(24, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (SE_hl): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(24, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): MFAB(\n",
       "        (hl_conv): Sequential(\n",
       "          (0): Conv2dReLU(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dReLU(\n",
       "            (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (SE_ll): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (SE_hl): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_souped_model(model,num_models=3, path =  your_current_dir.replace('MANet_training','models/MANet_souped')):\n",
    "        model_copy = deepcopy(model)\n",
    "        best_model = model_copy\n",
    "        best_model_name = path+'_3_models'\n",
    "        torch.save(best_model.state_dict(),best_model_name)\n",
    "        print('souped model saved!')\n",
    "        return best_model\n",
    "save_souped_model(souped_model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04b774be51d54e5dbe30cd67ea31e622": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a90d604aa1764e569754a2e6ba4f41c8",
      "placeholder": "​",
      "style": "IPY_MODEL_2cdafeca487e41c6820a7632d8411593",
      "value": "100%"
     }
    },
    "0ceccf3c8f7746e09357e26bbc2c84a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1858ca8d33de4f6dac7f14ffa6177298": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27f1a26db04f4a3fb2d6cc9e31dfe8cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cdafeca487e41c6820a7632d8411593": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53ff3c47b09a45b886f998d41eb73cb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1858ca8d33de4f6dac7f14ffa6177298",
      "max": 36804509,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ceccf3c8f7746e09357e26bbc2c84a1",
      "value": 36804509
     }
    },
    "676ba5cb97dd4da78395e2a711388bc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71bf8aeebddf49fa902f5cbb4e977ae2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f39d3cbbe44a453a99169236468e7a19",
      "placeholder": "​",
      "style": "IPY_MODEL_676ba5cb97dd4da78395e2a711388bc0",
      "value": " 35.1M/35.1M [00:00&lt;00:00, 110MB/s]"
     }
    },
    "8301d0859a154f0a951b6769c6ae6fbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_04b774be51d54e5dbe30cd67ea31e622",
       "IPY_MODEL_53ff3c47b09a45b886f998d41eb73cb9",
       "IPY_MODEL_71bf8aeebddf49fa902f5cbb4e977ae2"
      ],
      "layout": "IPY_MODEL_27f1a26db04f4a3fb2d6cc9e31dfe8cc"
     }
    },
    "a90d604aa1764e569754a2e6ba4f41c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f39d3cbbe44a453a99169236468e7a19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
